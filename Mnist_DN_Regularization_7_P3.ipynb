{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Mnist_DN_Regularization_7_P3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stiepan/MLCourse/blob/master/Mnist_DN_Regularization_7_P3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ziZ9i7tXbO1T"
      },
      "source": [
        "In this lab, you should try to implement some of the techniques discussed in the lecture.\n",
        "Here is a list of reasonable tasks.\n",
        " \n",
        "Easy:\n",
        " * L1 or L2 regularization (choose one)\n",
        " * momentum, Nesterov's momentum (choose one)\n",
        "\n",
        "Medium difficulty:\n",
        " * Adagrad, RMSProp (choose one)\n",
        " * dropout\n",
        " * data augmentation (tiny rotatations, up/down-scalings etc.)\n",
        "\n",
        "Try to test your network to see if these changes improve accuracy. They improve accuracy much more if you increase the layer size, and if you add more layers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "P22HqX9AbO1a",
        "colab": {}
      },
      "source": [
        "import random\n",
        "import numpy as np\n",
        "from torchvision import datasets, transforms"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "N9jGPaZhbO2B",
        "colab": {}
      },
      "source": [
        "# Let's read the mnist dataset\n",
        "\n",
        "def load_mnist(path='.'):\n",
        "    train_set = datasets.MNIST(path, train=True, download=True)\n",
        "    x_train = train_set.data.numpy()\n",
        "    _y_train = train_set.targets.numpy()\n",
        "    \n",
        "    test_set = datasets.MNIST(path, train=False, download=True)\n",
        "    x_test = test_set.data.numpy()\n",
        "    _y_test = test_set.targets.numpy()\n",
        "    \n",
        "    x_train = x_train.reshape((x_train.shape[0],28*28)) / 255.\n",
        "    x_test = x_test.reshape((x_test.shape[0],28*28)) / 255.\n",
        "\n",
        "    y_train = np.zeros((_y_train.shape[0], 10))\n",
        "    y_train[np.arange(_y_train.shape[0]), _y_train] = 1\n",
        "    \n",
        "    y_test = np.zeros((_y_test.shape[0], 10))\n",
        "    y_test[np.arange(_y_test.shape[0]), _y_test] = 1\n",
        "\n",
        "    return (x_train, y_train), (x_test, y_test)\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = load_mnist()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "w3gAyqw4bO1p",
        "colab": {}
      },
      "source": [
        "def sigmoid(z):\n",
        "    return 1.0/(1.0+np.exp(-z))\n",
        "\n",
        "def sigmoid_prime(z):\n",
        "    # Derivative of the sigmoid\n",
        "    return sigmoid(z)*(1-sigmoid(z))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FgEA2XRRbO2X",
        "outputId": "eb316b03-adb6-4245-e451-be0c23d4ff40",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# nestrov's momentum + l1 regularization\n",
        "\n",
        "class Network(object):\n",
        "    def __init__(self, sizes, momentum=0.):\n",
        "        # initialize biases and weights with random normal distr.\n",
        "        # weights are indexed by target node first\n",
        "        self.momentum = momentum\n",
        "        self.num_layers = len(sizes)\n",
        "        self.sizes = sizes\n",
        "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
        "        self.weights = [np.random.randn(y, x) \n",
        "                        for x, y in zip(sizes[:-1], sizes[1:])]\n",
        "        self.acc_w_grad = [np.zeros_like(w) for w in self.weights]\n",
        "        self.acc_b_grad = [np.zeros_like(b) for b in self.biases]\n",
        "    \n",
        "    @property\n",
        "    def final_biases(self):\n",
        "      return [\n",
        "        b - self.momentum * v \n",
        "        for b, v in zip(self.biases, self.acc_b_grad)\n",
        "      ]\n",
        "\n",
        "    @property\n",
        "    def final_weights(self):\n",
        "      return [\n",
        "        w - self.momentum * v\n",
        "        for w, v in zip(self.weights, self.acc_w_grad)\n",
        "      ]\n",
        "    \n",
        "    def feedforward(self, a):\n",
        "        # Run the network on a batch\n",
        "        a = a.T\n",
        "        for b, w in zip(self.final_biases, self.final_weights):\n",
        "            a = sigmoid(np.matmul(w, a)+b)\n",
        "        return a\n",
        "\n",
        "    def l1_reg(self, lmbda):\n",
        "      ds = [lmbda * np.sign(w) for w in self.weights]\n",
        "      eliminated = [np.abs(w) <= np.abs(d) for w, d in zip(self.weights, ds)]\n",
        "      self.weights = [w - d for w, d in zip(self.weights, ds)]\n",
        "      for w, z in zip(self.weights, eliminated):\n",
        "        w[z] = 0.\n",
        "\n",
        "    \n",
        "    def update_mini_batch(self, mini_batch, eta, lmbda=None):\n",
        "        # Update networks weights and biases by applying a single step\n",
        "        # of gradient descent using backpropagation to compute the gradient.\n",
        "        # The gradient is computed for a mini_batch which is as in tensorflow API.\n",
        "        # eta is the learning rate\n",
        "\n",
        "        seta = eta/len(mini_batch[0])\n",
        "        if lmbda is not None:\n",
        "          self.l1_reg(seta * lmbda)\n",
        "        \n",
        "        acc_w_grad = self.acc_w_grad\n",
        "        acc_b_grad = self.acc_b_grad\n",
        "\n",
        "        nabla_b, nabla_w = self.backprop(mini_batch[0].T,mini_batch[1].T)\n",
        "\n",
        "        self.acc_w_grad = [\n",
        "          self.momentum * vt - seta * nw\n",
        "          for vt, nw in zip(acc_w_grad, nabla_w)\n",
        "        ]\n",
        "        self.acc_b_grad = [\n",
        "          self.momentum * vt - seta * nb\n",
        "          for vt, nb in zip(acc_b_grad, nabla_b)\n",
        "        ]\n",
        "\n",
        "        self.weights = [\n",
        "          w + (1 + self.momentum) * v_next - self.momentum * v\n",
        "          for w, v, v_next in zip(self.weights, acc_w_grad, self.acc_w_grad)\n",
        "        ]\n",
        "        self.biases = [\n",
        "          b + (1 + self.momentum) * v_next - self.momentum * v\n",
        "          for b, v, v_next in zip(self.biases, acc_b_grad, self.acc_b_grad)\n",
        "        ]\n",
        "\n",
        "        \n",
        "    def backprop(self, x, y):\n",
        "        # For a single input (x,y) return a pair of lists.\n",
        "        # First contains gradients over biases, second over weights.\n",
        "        g = x\n",
        "        gs = [g] # list to store all the gs, layer by layer\n",
        "        fs = [] # list to store all the fs, layer by layer\n",
        "        for b, w in zip(self.biases, self.weights):\n",
        "            f = np.dot(w, g)+b\n",
        "            fs.append(f)\n",
        "            g = sigmoid(f)\n",
        "            gs.append(g)\n",
        "        # backward pass <- both steps at once\n",
        "        dLdg = self.cost_derivative(gs[-1], y)\n",
        "        dLdfs = []\n",
        "        for w,g in reversed(list(zip(self.weights,gs[1:]))):\n",
        "            dLdf = np.multiply(dLdg,np.multiply(g,1-g))\n",
        "            dLdfs.append(dLdf)\n",
        "            dLdg = np.matmul(w.T, dLdf)\n",
        "        \n",
        "        dLdWs = [np.matmul(dLdf,g.T) for dLdf,g in zip(reversed(dLdfs),gs[:-1])] # automatic here\n",
        "        dLdBs = [np.sum(dLdf,axis=1).reshape(dLdf.shape[0],1) for dLdf in reversed(dLdfs)] # CHANGE: Need to sum here\n",
        "        return (dLdBs,dLdWs)\n",
        "\n",
        "    def evaluate(self, test_data):\n",
        "        # Count the number of correct answers for test_data\n",
        "        pred = np.argmax(self.feedforward(test_data[0]),axis=0)\n",
        "        corr = np.argmax(test_data[1],axis=1).T\n",
        "        return np.mean(pred==corr)\n",
        "    \n",
        "    def cost_derivative(self, output_activations, y):\n",
        "        return (output_activations-y) \n",
        "    \n",
        "    def SGD(self, training_data, epochs, mini_batch_size, eta, test_data=None, lmbda=None):\n",
        "        x_train, y_train = training_data\n",
        "        if test_data:\n",
        "            x_test, y_test = test_data\n",
        "        for j in range(epochs):\n",
        "            for i in range(x_train.shape[0] // mini_batch_size):\n",
        "                x_mini_batch = x_train[(mini_batch_size*i):(mini_batch_size*(i+1))]\n",
        "                y_mini_batch = y_train[(mini_batch_size*i):(mini_batch_size*(i+1))]\n",
        "                self.update_mini_batch((x_mini_batch, y_mini_batch), eta, lmbda)\n",
        "            if test_data:\n",
        "                print(\"Epoch: {0}, Accuracy: {1}\".format(j, self.evaluate((x_test, y_test))))\n",
        "            else:\n",
        "                print(\"Epoch: {0}\".format(j))\n",
        "\n",
        "\n",
        "network = Network([784,30,10], 0.25)\n",
        "network.SGD((x_train, y_train), epochs=200, mini_batch_size=100, eta=3.0, test_data=(x_test, y_test), lmbda=0.0015)\n",
        "\n"
      ],
      "execution_count": 148,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0, Accuracy: 0.6906\n",
            "Epoch: 1, Accuracy: 0.862\n",
            "Epoch: 2, Accuracy: 0.8959\n",
            "Epoch: 3, Accuracy: 0.9068\n",
            "Epoch: 4, Accuracy: 0.9123\n",
            "Epoch: 5, Accuracy: 0.9171\n",
            "Epoch: 6, Accuracy: 0.9224\n",
            "Epoch: 7, Accuracy: 0.9258\n",
            "Epoch: 8, Accuracy: 0.9287\n",
            "Epoch: 9, Accuracy: 0.9309\n",
            "Epoch: 10, Accuracy: 0.9334\n",
            "Epoch: 11, Accuracy: 0.9357\n",
            "Epoch: 12, Accuracy: 0.9365\n",
            "Epoch: 13, Accuracy: 0.9378\n",
            "Epoch: 14, Accuracy: 0.9396\n",
            "Epoch: 15, Accuracy: 0.9401\n",
            "Epoch: 16, Accuracy: 0.9412\n",
            "Epoch: 17, Accuracy: 0.9422\n",
            "Epoch: 18, Accuracy: 0.9424\n",
            "Epoch: 19, Accuracy: 0.9438\n",
            "Epoch: 20, Accuracy: 0.9451\n",
            "Epoch: 21, Accuracy: 0.9457\n",
            "Epoch: 22, Accuracy: 0.9465\n",
            "Epoch: 23, Accuracy: 0.9466\n",
            "Epoch: 24, Accuracy: 0.9473\n",
            "Epoch: 25, Accuracy: 0.9476\n",
            "Epoch: 26, Accuracy: 0.9481\n",
            "Epoch: 27, Accuracy: 0.9483\n",
            "Epoch: 28, Accuracy: 0.9488\n",
            "Epoch: 29, Accuracy: 0.9498\n",
            "Epoch: 30, Accuracy: 0.9499\n",
            "Epoch: 31, Accuracy: 0.9507\n",
            "Epoch: 32, Accuracy: 0.9509\n",
            "Epoch: 33, Accuracy: 0.9522\n",
            "Epoch: 34, Accuracy: 0.9522\n",
            "Epoch: 35, Accuracy: 0.9528\n",
            "Epoch: 36, Accuracy: 0.9527\n",
            "Epoch: 37, Accuracy: 0.9526\n",
            "Epoch: 38, Accuracy: 0.9524\n",
            "Epoch: 39, Accuracy: 0.9525\n",
            "Epoch: 40, Accuracy: 0.953\n",
            "Epoch: 41, Accuracy: 0.9537\n",
            "Epoch: 42, Accuracy: 0.9539\n",
            "Epoch: 43, Accuracy: 0.9543\n",
            "Epoch: 44, Accuracy: 0.9546\n",
            "Epoch: 45, Accuracy: 0.9546\n",
            "Epoch: 46, Accuracy: 0.955\n",
            "Epoch: 47, Accuracy: 0.9548\n",
            "Epoch: 48, Accuracy: 0.9551\n",
            "Epoch: 49, Accuracy: 0.9554\n",
            "Epoch: 50, Accuracy: 0.956\n",
            "Epoch: 51, Accuracy: 0.9561\n",
            "Epoch: 52, Accuracy: 0.9566\n",
            "Epoch: 53, Accuracy: 0.9565\n",
            "Epoch: 54, Accuracy: 0.9562\n",
            "Epoch: 55, Accuracy: 0.9562\n",
            "Epoch: 56, Accuracy: 0.9562\n",
            "Epoch: 57, Accuracy: 0.9559\n",
            "Epoch: 58, Accuracy: 0.9559\n",
            "Epoch: 59, Accuracy: 0.9558\n",
            "Epoch: 60, Accuracy: 0.9558\n",
            "Epoch: 61, Accuracy: 0.9561\n",
            "Epoch: 62, Accuracy: 0.9558\n",
            "Epoch: 63, Accuracy: 0.9559\n",
            "Epoch: 64, Accuracy: 0.9557\n",
            "Epoch: 65, Accuracy: 0.9558\n",
            "Epoch: 66, Accuracy: 0.9556\n",
            "Epoch: 67, Accuracy: 0.9563\n",
            "Epoch: 68, Accuracy: 0.9565\n",
            "Epoch: 69, Accuracy: 0.957\n",
            "Epoch: 70, Accuracy: 0.9576\n",
            "Epoch: 71, Accuracy: 0.9576\n",
            "Epoch: 72, Accuracy: 0.9578\n",
            "Epoch: 73, Accuracy: 0.9579\n",
            "Epoch: 74, Accuracy: 0.9583\n",
            "Epoch: 75, Accuracy: 0.9588\n",
            "Epoch: 76, Accuracy: 0.9589\n",
            "Epoch: 77, Accuracy: 0.9591\n",
            "Epoch: 78, Accuracy: 0.959\n",
            "Epoch: 79, Accuracy: 0.9593\n",
            "Epoch: 80, Accuracy: 0.9596\n",
            "Epoch: 81, Accuracy: 0.9597\n",
            "Epoch: 82, Accuracy: 0.9597\n",
            "Epoch: 83, Accuracy: 0.9597\n",
            "Epoch: 84, Accuracy: 0.9595\n",
            "Epoch: 85, Accuracy: 0.9596\n",
            "Epoch: 86, Accuracy: 0.9598\n",
            "Epoch: 87, Accuracy: 0.9601\n",
            "Epoch: 88, Accuracy: 0.9603\n",
            "Epoch: 89, Accuracy: 0.9606\n",
            "Epoch: 90, Accuracy: 0.9603\n",
            "Epoch: 91, Accuracy: 0.9601\n",
            "Epoch: 92, Accuracy: 0.9604\n",
            "Epoch: 93, Accuracy: 0.9606\n",
            "Epoch: 94, Accuracy: 0.9607\n",
            "Epoch: 95, Accuracy: 0.9609\n",
            "Epoch: 96, Accuracy: 0.9611\n",
            "Epoch: 97, Accuracy: 0.9613\n",
            "Epoch: 98, Accuracy: 0.9615\n",
            "Epoch: 99, Accuracy: 0.9615\n",
            "Epoch: 100, Accuracy: 0.9614\n",
            "Epoch: 101, Accuracy: 0.9614\n",
            "Epoch: 102, Accuracy: 0.9613\n",
            "Epoch: 103, Accuracy: 0.9611\n",
            "Epoch: 104, Accuracy: 0.9612\n",
            "Epoch: 105, Accuracy: 0.9611\n",
            "Epoch: 106, Accuracy: 0.9608\n",
            "Epoch: 107, Accuracy: 0.9608\n",
            "Epoch: 108, Accuracy: 0.9611\n",
            "Epoch: 109, Accuracy: 0.9612\n",
            "Epoch: 110, Accuracy: 0.9609\n",
            "Epoch: 111, Accuracy: 0.961\n",
            "Epoch: 112, Accuracy: 0.9608\n",
            "Epoch: 113, Accuracy: 0.9603\n",
            "Epoch: 114, Accuracy: 0.9606\n",
            "Epoch: 115, Accuracy: 0.9607\n",
            "Epoch: 116, Accuracy: 0.9606\n",
            "Epoch: 117, Accuracy: 0.9612\n",
            "Epoch: 118, Accuracy: 0.9616\n",
            "Epoch: 119, Accuracy: 0.9619\n",
            "Epoch: 120, Accuracy: 0.9619\n",
            "Epoch: 121, Accuracy: 0.9618\n",
            "Epoch: 122, Accuracy: 0.9618\n",
            "Epoch: 123, Accuracy: 0.9618\n",
            "Epoch: 124, Accuracy: 0.9618\n",
            "Epoch: 125, Accuracy: 0.9612\n",
            "Epoch: 126, Accuracy: 0.9611\n",
            "Epoch: 127, Accuracy: 0.961\n",
            "Epoch: 128, Accuracy: 0.9613\n",
            "Epoch: 129, Accuracy: 0.9617\n",
            "Epoch: 130, Accuracy: 0.9616\n",
            "Epoch: 131, Accuracy: 0.9616\n",
            "Epoch: 132, Accuracy: 0.9615\n",
            "Epoch: 133, Accuracy: 0.9616\n",
            "Epoch: 134, Accuracy: 0.9616\n",
            "Epoch: 135, Accuracy: 0.9618\n",
            "Epoch: 136, Accuracy: 0.9618\n",
            "Epoch: 137, Accuracy: 0.962\n",
            "Epoch: 138, Accuracy: 0.9619\n",
            "Epoch: 139, Accuracy: 0.9619\n",
            "Epoch: 140, Accuracy: 0.962\n",
            "Epoch: 141, Accuracy: 0.9621\n",
            "Epoch: 142, Accuracy: 0.962\n",
            "Epoch: 143, Accuracy: 0.9617\n",
            "Epoch: 144, Accuracy: 0.9616\n",
            "Epoch: 145, Accuracy: 0.9618\n",
            "Epoch: 146, Accuracy: 0.9616\n",
            "Epoch: 147, Accuracy: 0.9618\n",
            "Epoch: 148, Accuracy: 0.9619\n",
            "Epoch: 149, Accuracy: 0.9619\n",
            "Epoch: 150, Accuracy: 0.962\n",
            "Epoch: 151, Accuracy: 0.962\n",
            "Epoch: 152, Accuracy: 0.962\n",
            "Epoch: 153, Accuracy: 0.9623\n",
            "Epoch: 154, Accuracy: 0.9623\n",
            "Epoch: 155, Accuracy: 0.9623\n",
            "Epoch: 156, Accuracy: 0.9623\n",
            "Epoch: 157, Accuracy: 0.9622\n",
            "Epoch: 158, Accuracy: 0.9622\n",
            "Epoch: 159, Accuracy: 0.9621\n",
            "Epoch: 160, Accuracy: 0.9619\n",
            "Epoch: 161, Accuracy: 0.9619\n",
            "Epoch: 162, Accuracy: 0.9617\n",
            "Epoch: 163, Accuracy: 0.9616\n",
            "Epoch: 164, Accuracy: 0.9619\n",
            "Epoch: 165, Accuracy: 0.9617\n",
            "Epoch: 166, Accuracy: 0.9615\n",
            "Epoch: 167, Accuracy: 0.9616\n",
            "Epoch: 168, Accuracy: 0.9615\n",
            "Epoch: 169, Accuracy: 0.9615\n",
            "Epoch: 170, Accuracy: 0.9615\n",
            "Epoch: 171, Accuracy: 0.9615\n",
            "Epoch: 172, Accuracy: 0.9616\n",
            "Epoch: 173, Accuracy: 0.9616\n",
            "Epoch: 174, Accuracy: 0.9616\n",
            "Epoch: 175, Accuracy: 0.9618\n",
            "Epoch: 176, Accuracy: 0.962\n",
            "Epoch: 177, Accuracy: 0.9621\n",
            "Epoch: 178, Accuracy: 0.962\n",
            "Epoch: 179, Accuracy: 0.962\n",
            "Epoch: 180, Accuracy: 0.9618\n",
            "Epoch: 181, Accuracy: 0.9619\n",
            "Epoch: 182, Accuracy: 0.962\n",
            "Epoch: 183, Accuracy: 0.962\n",
            "Epoch: 184, Accuracy: 0.9621\n",
            "Epoch: 185, Accuracy: 0.9622\n",
            "Epoch: 186, Accuracy: 0.9622\n",
            "Epoch: 187, Accuracy: 0.9621\n",
            "Epoch: 188, Accuracy: 0.9621\n",
            "Epoch: 189, Accuracy: 0.9621\n",
            "Epoch: 190, Accuracy: 0.9619\n",
            "Epoch: 191, Accuracy: 0.9618\n",
            "Epoch: 192, Accuracy: 0.9617\n",
            "Epoch: 193, Accuracy: 0.9616\n",
            "Epoch: 194, Accuracy: 0.9616\n",
            "Epoch: 195, Accuracy: 0.9615\n",
            "Epoch: 196, Accuracy: 0.9614\n",
            "Epoch: 197, Accuracy: 0.9614\n",
            "Epoch: 198, Accuracy: 0.9614\n",
            "Epoch: 199, Accuracy: 0.9612\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mYe_2XVpnf3R",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3d382395-6d98-4ca5-bd84-771d399e08cd"
      },
      "source": [
        "# RMSProp + l1 regularization\n",
        "\n",
        "class NetworkRMS(Network):\n",
        "\n",
        "    @property\n",
        "    def final_biases(self):\n",
        "      return self.biases\n",
        "\n",
        "    @property\n",
        "    def final_weights(self):\n",
        "      return self.weights\n",
        "    \n",
        "    def update_mini_batch(self, mini_batch, eta, lmbda=None):\n",
        "        # Update networks weights and biases by applying a single step\n",
        "        # of gradient descent using backpropagation to compute the gradient.\n",
        "        # The gradient is computed for a mini_batch which is as in tensorflow API.\n",
        "        # eta is the learning rate\n",
        "\n",
        "        eps = 1e-8\n",
        "        seta = eta/len(mini_batch[0])\n",
        "        \n",
        "        if lmbda is not None:\n",
        "          self.l1_reg(seta * lmbda)\n",
        "\n",
        "        nabla_b, nabla_w = self.backprop(mini_batch[0].T,mini_batch[1].T)\n",
        "\n",
        "        self.acc_w_grad = [\n",
        "          self.momentum * vt + (1-self.momentum) * (nw ** 2)\n",
        "          for vt, nw in zip(self.acc_w_grad, nabla_w)\n",
        "        ]\n",
        "        self.acc_b_grad = [\n",
        "          self.momentum * vt + (1-self.momentum) * (nb ** 2)\n",
        "          for vt, nb in zip(self.acc_b_grad, nabla_b)\n",
        "        ]\n",
        "\n",
        "        self.weights = [\n",
        "          w - seta * np.divide(nw, np.sqrt(v + eps))\n",
        "          for w, v, nw in zip(self.weights, self.acc_w_grad, nabla_w)\n",
        "        ]\n",
        "        self.biases = [\n",
        "          b - seta * np.divide(nb, np.sqrt(v + eps))\n",
        "          for b, v, nb in zip(self.biases, self.acc_b_grad, nabla_b)\n",
        "        ]\n",
        "\n",
        "\n",
        "network2 = NetworkRMS([784,30,10], 0.9)\n",
        "network2.SGD((x_train, y_train), epochs=200, mini_batch_size=100, eta=0.08, test_data=(x_test, y_test), lmbda=0.03)\n",
        "\n"
      ],
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0, Accuracy: 0.5956\n",
            "Epoch: 1, Accuracy: 0.7551\n",
            "Epoch: 2, Accuracy: 0.8463\n",
            "Epoch: 3, Accuracy: 0.8737\n",
            "Epoch: 4, Accuracy: 0.8867\n",
            "Epoch: 5, Accuracy: 0.8959\n",
            "Epoch: 6, Accuracy: 0.9031\n",
            "Epoch: 7, Accuracy: 0.9077\n",
            "Epoch: 8, Accuracy: 0.9131\n",
            "Epoch: 9, Accuracy: 0.9164\n",
            "Epoch: 10, Accuracy: 0.9199\n",
            "Epoch: 11, Accuracy: 0.923\n",
            "Epoch: 12, Accuracy: 0.9249\n",
            "Epoch: 13, Accuracy: 0.9273\n",
            "Epoch: 14, Accuracy: 0.9286\n",
            "Epoch: 15, Accuracy: 0.93\n",
            "Epoch: 16, Accuracy: 0.9319\n",
            "Epoch: 17, Accuracy: 0.9333\n",
            "Epoch: 18, Accuracy: 0.9344\n",
            "Epoch: 19, Accuracy: 0.9354\n",
            "Epoch: 20, Accuracy: 0.9369\n",
            "Epoch: 21, Accuracy: 0.9376\n",
            "Epoch: 22, Accuracy: 0.9387\n",
            "Epoch: 23, Accuracy: 0.9394\n",
            "Epoch: 24, Accuracy: 0.9401\n",
            "Epoch: 25, Accuracy: 0.94\n",
            "Epoch: 26, Accuracy: 0.9411\n",
            "Epoch: 27, Accuracy: 0.9414\n",
            "Epoch: 28, Accuracy: 0.9425\n",
            "Epoch: 29, Accuracy: 0.943\n",
            "Epoch: 30, Accuracy: 0.9433\n",
            "Epoch: 31, Accuracy: 0.9437\n",
            "Epoch: 32, Accuracy: 0.944\n",
            "Epoch: 33, Accuracy: 0.9446\n",
            "Epoch: 34, Accuracy: 0.945\n",
            "Epoch: 35, Accuracy: 0.9454\n",
            "Epoch: 36, Accuracy: 0.9462\n",
            "Epoch: 37, Accuracy: 0.9471\n",
            "Epoch: 38, Accuracy: 0.9475\n",
            "Epoch: 39, Accuracy: 0.9476\n",
            "Epoch: 40, Accuracy: 0.9478\n",
            "Epoch: 41, Accuracy: 0.9482\n",
            "Epoch: 42, Accuracy: 0.9482\n",
            "Epoch: 43, Accuracy: 0.948\n",
            "Epoch: 44, Accuracy: 0.9482\n",
            "Epoch: 45, Accuracy: 0.9492\n",
            "Epoch: 46, Accuracy: 0.9496\n",
            "Epoch: 47, Accuracy: 0.9499\n",
            "Epoch: 48, Accuracy: 0.9505\n",
            "Epoch: 49, Accuracy: 0.9515\n",
            "Epoch: 50, Accuracy: 0.9516\n",
            "Epoch: 51, Accuracy: 0.9522\n",
            "Epoch: 52, Accuracy: 0.9525\n",
            "Epoch: 53, Accuracy: 0.9529\n",
            "Epoch: 54, Accuracy: 0.9536\n",
            "Epoch: 55, Accuracy: 0.9537\n",
            "Epoch: 56, Accuracy: 0.9541\n",
            "Epoch: 57, Accuracy: 0.9549\n",
            "Epoch: 58, Accuracy: 0.9549\n",
            "Epoch: 59, Accuracy: 0.9548\n",
            "Epoch: 60, Accuracy: 0.9549\n",
            "Epoch: 61, Accuracy: 0.9554\n",
            "Epoch: 62, Accuracy: 0.9555\n",
            "Epoch: 63, Accuracy: 0.956\n",
            "Epoch: 64, Accuracy: 0.9564\n",
            "Epoch: 65, Accuracy: 0.9566\n",
            "Epoch: 66, Accuracy: 0.9567\n",
            "Epoch: 67, Accuracy: 0.957\n",
            "Epoch: 68, Accuracy: 0.9573\n",
            "Epoch: 69, Accuracy: 0.9571\n",
            "Epoch: 70, Accuracy: 0.9571\n",
            "Epoch: 71, Accuracy: 0.9574\n",
            "Epoch: 72, Accuracy: 0.9574\n",
            "Epoch: 73, Accuracy: 0.9575\n",
            "Epoch: 74, Accuracy: 0.9573\n",
            "Epoch: 75, Accuracy: 0.9573\n",
            "Epoch: 76, Accuracy: 0.9574\n",
            "Epoch: 77, Accuracy: 0.9574\n",
            "Epoch: 78, Accuracy: 0.9575\n",
            "Epoch: 79, Accuracy: 0.9578\n",
            "Epoch: 80, Accuracy: 0.9577\n",
            "Epoch: 81, Accuracy: 0.9575\n",
            "Epoch: 82, Accuracy: 0.9578\n",
            "Epoch: 83, Accuracy: 0.9582\n",
            "Epoch: 84, Accuracy: 0.9584\n",
            "Epoch: 85, Accuracy: 0.9588\n",
            "Epoch: 86, Accuracy: 0.9591\n",
            "Epoch: 87, Accuracy: 0.9587\n",
            "Epoch: 88, Accuracy: 0.9587\n",
            "Epoch: 89, Accuracy: 0.9585\n",
            "Epoch: 90, Accuracy: 0.9586\n",
            "Epoch: 91, Accuracy: 0.9587\n",
            "Epoch: 92, Accuracy: 0.9588\n",
            "Epoch: 93, Accuracy: 0.959\n",
            "Epoch: 94, Accuracy: 0.9592\n",
            "Epoch: 95, Accuracy: 0.9593\n",
            "Epoch: 96, Accuracy: 0.9593\n",
            "Epoch: 97, Accuracy: 0.9594\n",
            "Epoch: 98, Accuracy: 0.9596\n",
            "Epoch: 99, Accuracy: 0.9596\n",
            "Epoch: 100, Accuracy: 0.9595\n",
            "Epoch: 101, Accuracy: 0.9596\n",
            "Epoch: 102, Accuracy: 0.9595\n",
            "Epoch: 103, Accuracy: 0.9598\n",
            "Epoch: 104, Accuracy: 0.9599\n",
            "Epoch: 105, Accuracy: 0.9599\n",
            "Epoch: 106, Accuracy: 0.9596\n",
            "Epoch: 107, Accuracy: 0.9597\n",
            "Epoch: 108, Accuracy: 0.9598\n",
            "Epoch: 109, Accuracy: 0.96\n",
            "Epoch: 110, Accuracy: 0.9603\n",
            "Epoch: 111, Accuracy: 0.9602\n",
            "Epoch: 112, Accuracy: 0.9604\n",
            "Epoch: 113, Accuracy: 0.9603\n",
            "Epoch: 114, Accuracy: 0.9604\n",
            "Epoch: 115, Accuracy: 0.9604\n",
            "Epoch: 116, Accuracy: 0.9604\n",
            "Epoch: 117, Accuracy: 0.9607\n",
            "Epoch: 118, Accuracy: 0.9608\n",
            "Epoch: 119, Accuracy: 0.9608\n",
            "Epoch: 120, Accuracy: 0.9612\n",
            "Epoch: 121, Accuracy: 0.9613\n",
            "Epoch: 122, Accuracy: 0.9615\n",
            "Epoch: 123, Accuracy: 0.9615\n",
            "Epoch: 124, Accuracy: 0.9615\n",
            "Epoch: 125, Accuracy: 0.9616\n",
            "Epoch: 126, Accuracy: 0.9619\n",
            "Epoch: 127, Accuracy: 0.9617\n",
            "Epoch: 128, Accuracy: 0.9619\n",
            "Epoch: 129, Accuracy: 0.9619\n",
            "Epoch: 130, Accuracy: 0.962\n",
            "Epoch: 131, Accuracy: 0.962\n",
            "Epoch: 132, Accuracy: 0.962\n",
            "Epoch: 133, Accuracy: 0.962\n",
            "Epoch: 134, Accuracy: 0.9622\n",
            "Epoch: 135, Accuracy: 0.9622\n",
            "Epoch: 136, Accuracy: 0.9623\n",
            "Epoch: 137, Accuracy: 0.9624\n",
            "Epoch: 138, Accuracy: 0.9624\n",
            "Epoch: 139, Accuracy: 0.9625\n",
            "Epoch: 140, Accuracy: 0.9621\n",
            "Epoch: 141, Accuracy: 0.9621\n",
            "Epoch: 142, Accuracy: 0.9624\n",
            "Epoch: 143, Accuracy: 0.9624\n",
            "Epoch: 144, Accuracy: 0.9625\n",
            "Epoch: 145, Accuracy: 0.9627\n",
            "Epoch: 146, Accuracy: 0.9629\n",
            "Epoch: 147, Accuracy: 0.963\n",
            "Epoch: 148, Accuracy: 0.963\n",
            "Epoch: 149, Accuracy: 0.9631\n",
            "Epoch: 150, Accuracy: 0.9632\n",
            "Epoch: 151, Accuracy: 0.9635\n",
            "Epoch: 152, Accuracy: 0.9633\n",
            "Epoch: 153, Accuracy: 0.9633\n",
            "Epoch: 154, Accuracy: 0.9634\n",
            "Epoch: 155, Accuracy: 0.9636\n",
            "Epoch: 156, Accuracy: 0.9637\n",
            "Epoch: 157, Accuracy: 0.9639\n",
            "Epoch: 158, Accuracy: 0.9642\n",
            "Epoch: 159, Accuracy: 0.9644\n",
            "Epoch: 160, Accuracy: 0.9646\n",
            "Epoch: 161, Accuracy: 0.9647\n",
            "Epoch: 162, Accuracy: 0.9647\n",
            "Epoch: 163, Accuracy: 0.9647\n",
            "Epoch: 164, Accuracy: 0.965\n",
            "Epoch: 165, Accuracy: 0.9649\n",
            "Epoch: 166, Accuracy: 0.965\n",
            "Epoch: 167, Accuracy: 0.965\n",
            "Epoch: 168, Accuracy: 0.9651\n",
            "Epoch: 169, Accuracy: 0.9652\n",
            "Epoch: 170, Accuracy: 0.9652\n",
            "Epoch: 171, Accuracy: 0.9656\n",
            "Epoch: 172, Accuracy: 0.9656\n",
            "Epoch: 173, Accuracy: 0.9655\n",
            "Epoch: 174, Accuracy: 0.9655\n",
            "Epoch: 175, Accuracy: 0.9654\n",
            "Epoch: 176, Accuracy: 0.9653\n",
            "Epoch: 177, Accuracy: 0.9653\n",
            "Epoch: 178, Accuracy: 0.9657\n",
            "Epoch: 179, Accuracy: 0.9658\n",
            "Epoch: 180, Accuracy: 0.9659\n",
            "Epoch: 181, Accuracy: 0.9659\n",
            "Epoch: 182, Accuracy: 0.9658\n",
            "Epoch: 183, Accuracy: 0.9659\n",
            "Epoch: 184, Accuracy: 0.9659\n",
            "Epoch: 185, Accuracy: 0.9659\n",
            "Epoch: 186, Accuracy: 0.9659\n",
            "Epoch: 187, Accuracy: 0.9659\n",
            "Epoch: 188, Accuracy: 0.9659\n",
            "Epoch: 189, Accuracy: 0.9659\n",
            "Epoch: 190, Accuracy: 0.9657\n",
            "Epoch: 191, Accuracy: 0.9657\n",
            "Epoch: 192, Accuracy: 0.9658\n",
            "Epoch: 193, Accuracy: 0.966\n",
            "Epoch: 194, Accuracy: 0.966\n",
            "Epoch: 195, Accuracy: 0.9659\n",
            "Epoch: 196, Accuracy: 0.9659\n",
            "Epoch: 197, Accuracy: 0.9659\n",
            "Epoch: 198, Accuracy: 0.9658\n",
            "Epoch: 199, Accuracy: 0.9658\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Keqd7kOsS9Gs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}