{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MNIST_Neural_Network_6_P3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stiepan/MLCourse/blob/master/MNIST_Neural_Network_6_P3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6y4l5BmxTNNU",
        "colab": {}
      },
      "source": [
        "import random\n",
        "import numpy as np\n",
        "from torchvision import datasets, transforms"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uutaqUkuVAuF",
        "colab": {}
      },
      "source": [
        "# Let's read the mnist dataset\n",
        "\n",
        "def load_mnist(path='.'):\n",
        "    train_set = datasets.MNIST(path, train=True, download=True)\n",
        "    x_train = train_set.data.numpy()\n",
        "    _y_train = train_set.targets.numpy()\n",
        "    \n",
        "    test_set = datasets.MNIST(path, train=False, download=True)\n",
        "    x_test = test_set.data.numpy()\n",
        "    _y_test = test_set.targets.numpy()\n",
        "    \n",
        "    x_train = x_train / 255.\n",
        "    x_test = x_test / 255.\n",
        "\n",
        "    y_train = np.zeros((_y_train.shape[0], 10))\n",
        "    y_train[np.arange(_y_train.shape[0]), _y_train] = 1\n",
        "    \n",
        "    y_test = np.zeros((_y_test.shape[0], 10))\n",
        "    y_test[np.arange(_y_test.shape[0]), _y_test] = 1\n",
        "\n",
        "    return (x_train, y_train), (x_test, y_test)\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = load_mnist()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "T5PPE1ldTNNx"
      },
      "source": [
        "In this exercise your task is to fill in the gaps in this code by implementing the backpropagation algorithm\n",
        "Once this is done, you can run the network on the MNIST example and see how it performs. Feel free to play with the parameters.\n",
        "\n",
        "If you found this task too easy, try to implement a \"fully vectorized\" version, i.e. one using matrix operations instead of going over examples one by one."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OsCgwvfHTNN0",
        "outputId": "37b1da82-ad93-43b9-d2d2-200974d25fa4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        }
      },
      "source": [
        "def sigmoid(z):\n",
        "    return 1.0/(1.0+np.exp(-z))\n",
        "\n",
        "def sigmoid_prime(z):\n",
        "    # Derivative of the sigmoid\n",
        "    return sigmoid(z)*(1-sigmoid(z))\n",
        "\n",
        "class Network(object):\n",
        "    def __init__(self, sizes):\n",
        "        # initialize biases and weights with random normal distr.\n",
        "        # weights are indexed by target node first\n",
        "        self.num_layers = len(sizes)\n",
        "        self.sizes = sizes\n",
        "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
        "        self.weights = [np.random.randn(y, x) \n",
        "                        for x, y in zip(sizes[:-1], sizes[1:])]\n",
        "    def feedforward(self, a):\n",
        "        # Run the network on a single case\n",
        "        for b, w in zip(self.biases, self.weights):\n",
        "            a = sigmoid(np.dot(w, a)+b)\n",
        "        return a\n",
        "    \n",
        "    def update_mini_batch(self, x_mini_batch, y_mini_batch, eta):\n",
        "        # Update networks weights and biases by applying a single step\n",
        "        # of gradient descent using backpropagation to compute the gradient.\n",
        "        # The gradient is computed for a mini_batch.\n",
        "        # eta is the learning rate\n",
        "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
        "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
        "        for x, y in zip(x_mini_batch, y_mini_batch):\n",
        "            delta_nabla_b, delta_nabla_w = self.backprop(x.reshape(784,1), y.reshape(10,1))\n",
        "            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
        "            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
        "        self.weights = [w-(eta/len(x_mini_batch))*nw \n",
        "                        for w, nw in zip(self.weights, nabla_w)]\n",
        "        self.biases = [b-(eta/len(x_mini_batch))*nb \n",
        "                       for b, nb in zip(self.biases, nabla_b)]\n",
        "        \n",
        "    def backprop(self, x, y):\n",
        "        # For a single input (x,y) return a tuple of lists.\n",
        "        # First contains gradients over biases, second over weights.\n",
        "        \n",
        "        # First initialize the list of gradient arrays\n",
        "        delta_nabla_b = [np.zeros_like(p) for p in self.biases]\n",
        "        delta_nabla_w = [np.zeros_like(p) for p in self.weights]\n",
        "\n",
        "        fs = [None]\n",
        "        gs = [x]\n",
        "        for b, w in zip(self.biases, self.weights):\n",
        "          fs.append(w @ gs[-1] + b)\n",
        "          gs.append(sigmoid(fs[-1]))\n",
        "        \n",
        "        next_layer_der = self.cost_derivative(gs[-1], y)\n",
        "        for i in reversed(range(0, len(self.weights))):\n",
        "          delta_nabla_b[i] = next_layer_der * sigmoid_prime(fs[i + 1])\n",
        "          delta_nabla_w[i] = np.outer(delta_nabla_b[i], gs[i])\n",
        "          next_layer_der = self.weights[i].T @ delta_nabla_b[i]\n",
        "        \n",
        "        return delta_nabla_b, delta_nabla_w\n",
        "\n",
        "    def evaluate(self, x_test_data, y_test_data):\n",
        "        # Count the number of correct answers for test_data\n",
        "        test_results = [(np.argmax(self.feedforward(x_test_data[i].reshape(784,1))), np.argmax(y_test_data[i]))\n",
        "                        for i in range(len(x_test_data))]\n",
        "        # return accuracy\n",
        "        return np.mean([int(x == y) for (x, y) in test_results])\n",
        "    \n",
        "    def cost_derivative(self, output_activations, y):\n",
        "        return (output_activations-y) \n",
        "    \n",
        "    def SGD(self, training_data, epochs, mini_batch_size, eta, test_data=None):\n",
        "        x_train, y_train = training_data\n",
        "        if test_data:\n",
        "            x_test, y_test = test_data\n",
        "        for j in range(epochs):\n",
        "            for i in range(x_train.shape[0] // mini_batch_size):\n",
        "                x_mini_batch = x_train[i*mini_batch_size:(i*mini_batch_size + mini_batch_size)] \n",
        "                y_mini_batch = y_train[i*mini_batch_size:(i*mini_batch_size + mini_batch_size)] \n",
        "                self.update_mini_batch(x_mini_batch, y_mini_batch, eta)\n",
        "            if test_data:\n",
        "                print(\"Epoch: {0}, Accuracy: {1}\".format(j, self.evaluate(x_test, y_test)))\n",
        "            else:\n",
        "                print(\"Epoch: {0}\".format(j))\n",
        "\n",
        "\n",
        "network = Network([784,30,10])\n",
        "network.SGD((x_train, y_train), epochs=50, mini_batch_size=100, eta=3., test_data=(x_test, y_test))\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0, Accuracy: 0.796\n",
            "Epoch: 1, Accuracy: 0.8607\n",
            "Epoch: 2, Accuracy: 0.8855\n",
            "Epoch: 3, Accuracy: 0.8986\n",
            "Epoch: 4, Accuracy: 0.9045\n",
            "Epoch: 5, Accuracy: 0.9093\n",
            "Epoch: 6, Accuracy: 0.9132\n",
            "Epoch: 7, Accuracy: 0.9165\n",
            "Epoch: 8, Accuracy: 0.9192\n",
            "Epoch: 9, Accuracy: 0.921\n",
            "Epoch: 10, Accuracy: 0.923\n",
            "Epoch: 11, Accuracy: 0.9244\n",
            "Epoch: 12, Accuracy: 0.9259\n",
            "Epoch: 13, Accuracy: 0.9278\n",
            "Epoch: 14, Accuracy: 0.9286\n",
            "Epoch: 15, Accuracy: 0.9298\n",
            "Epoch: 16, Accuracy: 0.9308\n",
            "Epoch: 17, Accuracy: 0.9321\n",
            "Epoch: 18, Accuracy: 0.9332\n",
            "Epoch: 19, Accuracy: 0.9337\n",
            "Epoch: 20, Accuracy: 0.9349\n",
            "Epoch: 21, Accuracy: 0.935\n",
            "Epoch: 22, Accuracy: 0.9356\n",
            "Epoch: 23, Accuracy: 0.9361\n",
            "Epoch: 24, Accuracy: 0.9362\n",
            "Epoch: 25, Accuracy: 0.9369\n",
            "Epoch: 26, Accuracy: 0.937\n",
            "Epoch: 27, Accuracy: 0.938\n",
            "Epoch: 28, Accuracy: 0.9386\n",
            "Epoch: 29, Accuracy: 0.9389\n",
            "Epoch: 30, Accuracy: 0.9392\n",
            "Epoch: 31, Accuracy: 0.9394\n",
            "Epoch: 32, Accuracy: 0.9397\n",
            "Epoch: 33, Accuracy: 0.9397\n",
            "Epoch: 34, Accuracy: 0.9405\n",
            "Epoch: 35, Accuracy: 0.94\n",
            "Epoch: 36, Accuracy: 0.94\n",
            "Epoch: 37, Accuracy: 0.94\n",
            "Epoch: 38, Accuracy: 0.9399\n",
            "Epoch: 39, Accuracy: 0.9402\n",
            "Epoch: 40, Accuracy: 0.9401\n",
            "Epoch: 41, Accuracy: 0.9409\n",
            "Epoch: 42, Accuracy: 0.9412\n",
            "Epoch: 43, Accuracy: 0.9409\n",
            "Epoch: 44, Accuracy: 0.9413\n",
            "Epoch: 45, Accuracy: 0.9413\n",
            "Epoch: 46, Accuracy: 0.9417\n",
            "Epoch: 47, Accuracy: 0.9419\n",
            "Epoch: 48, Accuracy: 0.9424\n",
            "Epoch: 49, Accuracy: 0.9425\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4ks-sxtd6VrY",
        "outputId": "543fd136-8c20-491c-e872-aea5186c43c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "class NetworkM(object):\n",
        "    def __init__(self, sizes):\n",
        "        # initialize biases and weights with random normal distr.\n",
        "        # weights are indexed by target node first\n",
        "        self.num_layers = len(sizes)\n",
        "        self.sizes = sizes\n",
        "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
        "        self.weights = [np.random.randn(y, x) \n",
        "                        for x, y in zip(sizes[:-1], sizes[1:])]\n",
        "    def feedforward(self, X):\n",
        "        # Run the network on a single case\n",
        "        a = X.T\n",
        "        for b, w in zip(self.biases, self.weights):\n",
        "            a = sigmoid(np.dot(w, a)+b)\n",
        "        return a\n",
        "    \n",
        "    def update_mini_batch(self, x_mini_batch, y_mini_batch, eta):\n",
        "        # Update networks weights and biases by applying a single step\n",
        "        # of gradient descent using backpropagation to compute the gradient.\n",
        "        # The gradient is computed for a mini_batch.\n",
        "        # eta is the learning rate\n",
        "        nabla_b, nabla_w = self.backprop(x_mini_batch, y_mini_batch)\n",
        "        self.weights = [w-(eta/len(x_mini_batch))*nw \n",
        "                        for w, nw in zip(self.weights, nabla_w)]\n",
        "        self.biases = [b-(eta/len(x_mini_batch))*nb \n",
        "                       for b, nb in zip(self.biases, nabla_b)]\n",
        "        \n",
        "    def backprop(self, X, ys):\n",
        "        nabla_b = [None] * len(self.biases)\n",
        "        nabla_w = [None]* len(self.weights)\n",
        "\n",
        "        fs = [None]\n",
        "        gs = [X.T]\n",
        "        for b, w in zip(self.biases, self.weights):\n",
        "          fs.append(w @ gs[-1] + b)\n",
        "          gs.append(sigmoid(fs[-1]))\n",
        "        \n",
        "        next_layer_der = self.cost_derivative(gs[-1], ys)\n",
        "        for i in reversed(range(0, len(self.weights))):\n",
        "          node_der = next_layer_der * sigmoid_prime(fs[i + 1])\n",
        "          nabla_b[i] = node_der @ np.ones(X.shape[0])\n",
        "          nabla_w[i] = node_der @ gs[i].T\n",
        "          next_layer_der = self.weights[i].T @ node_der    \n",
        "        nabla_b = [b.reshape(b.shape[0], 1) for b in nabla_b]\n",
        "    \n",
        "        return nabla_b, nabla_w\n",
        "\n",
        "    def evaluate(self, x_test_data, y_test_data):\n",
        "        # Count the number of correct answers for test_data\n",
        "        ps = np.argmax(self.feedforward(x_test_data), axis=0)\n",
        "        ys = np.argmax(y_test_data, axis=1)\n",
        "        return np.mean(ps == ys)\n",
        "    \n",
        "    def cost_derivative(self, output_activations, y):\n",
        "        return (output_activations-y.T) \n",
        "    \n",
        "    def x_feed_shape(self, X):\n",
        "      samples = X.shape[0]\n",
        "      sample_len = X.shape[1:]\n",
        "      return X.reshape([samples, np.prod(sample_len)])\n",
        "\n",
        "    \n",
        "    def SGD(self, training_data, epochs, mini_batch_size, eta, test_data=None):\n",
        "        x_train, y_train = training_data\n",
        "        x_train = self.x_feed_shape(x_train)\n",
        "        if test_data:\n",
        "            x_test, y_test = test_data\n",
        "            x_test = self.x_feed_shape(x_test)\n",
        "        for j in range(epochs):\n",
        "            for i in range(x_train.shape[0] // mini_batch_size):\n",
        "                x_mini_batch = x_train[i*mini_batch_size:(i*mini_batch_size + mini_batch_size)] \n",
        "                y_mini_batch = y_train[i*mini_batch_size:(i*mini_batch_size + mini_batch_size)] \n",
        "                self.update_mini_batch(x_mini_batch, y_mini_batch, eta)\n",
        "            if test_data:\n",
        "                print(\"Epoch: {0}, Accuracy: {1}\".format(j, self.evaluate(x_test, y_test)))\n",
        "            else:\n",
        "                print(\"Epoch: {0}\".format(j))\n",
        "\n",
        "\n",
        "network_m = NetworkM([784,30,10])\n",
        "network_m.SGD((x_train, y_train), epochs=100, mini_batch_size=10, eta=10., test_data=(x_test, y_test))\n",
        "\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 0, Accuracy: 0.915\n",
            "Epoch: 1, Accuracy: 0.9222\n",
            "Epoch: 2, Accuracy: 0.9253\n",
            "Epoch: 3, Accuracy: 0.9363\n",
            "Epoch: 4, Accuracy: 0.9313\n",
            "Epoch: 5, Accuracy: 0.9438\n",
            "Epoch: 6, Accuracy: 0.942\n",
            "Epoch: 7, Accuracy: 0.9411\n",
            "Epoch: 8, Accuracy: 0.9438\n",
            "Epoch: 9, Accuracy: 0.9426\n",
            "Epoch: 10, Accuracy: 0.9413\n",
            "Epoch: 11, Accuracy: 0.9451\n",
            "Epoch: 12, Accuracy: 0.9417\n",
            "Epoch: 13, Accuracy: 0.9418\n",
            "Epoch: 14, Accuracy: 0.945\n",
            "Epoch: 15, Accuracy: 0.9477\n",
            "Epoch: 16, Accuracy: 0.9492\n",
            "Epoch: 17, Accuracy: 0.9492\n",
            "Epoch: 18, Accuracy: 0.9493\n",
            "Epoch: 19, Accuracy: 0.9528\n",
            "Epoch: 20, Accuracy: 0.9523\n",
            "Epoch: 21, Accuracy: 0.9513\n",
            "Epoch: 22, Accuracy: 0.9528\n",
            "Epoch: 23, Accuracy: 0.949\n",
            "Epoch: 24, Accuracy: 0.9519\n",
            "Epoch: 25, Accuracy: 0.9516\n",
            "Epoch: 26, Accuracy: 0.9519\n",
            "Epoch: 27, Accuracy: 0.9478\n",
            "Epoch: 28, Accuracy: 0.9495\n",
            "Epoch: 29, Accuracy: 0.9488\n",
            "Epoch: 30, Accuracy: 0.9504\n",
            "Epoch: 31, Accuracy: 0.9507\n",
            "Epoch: 32, Accuracy: 0.9533\n",
            "Epoch: 33, Accuracy: 0.9516\n",
            "Epoch: 34, Accuracy: 0.952\n",
            "Epoch: 35, Accuracy: 0.9506\n",
            "Epoch: 36, Accuracy: 0.9517\n",
            "Epoch: 37, Accuracy: 0.9531\n",
            "Epoch: 38, Accuracy: 0.9501\n",
            "Epoch: 39, Accuracy: 0.9454\n",
            "Epoch: 40, Accuracy: 0.9489\n",
            "Epoch: 41, Accuracy: 0.9512\n",
            "Epoch: 42, Accuracy: 0.9503\n",
            "Epoch: 43, Accuracy: 0.9495\n",
            "Epoch: 44, Accuracy: 0.9536\n",
            "Epoch: 45, Accuracy: 0.9531\n",
            "Epoch: 46, Accuracy: 0.9528\n",
            "Epoch: 47, Accuracy: 0.9539\n",
            "Epoch: 48, Accuracy: 0.952\n",
            "Epoch: 49, Accuracy: 0.9534\n",
            "Epoch: 50, Accuracy: 0.9521\n",
            "Epoch: 51, Accuracy: 0.9487\n",
            "Epoch: 52, Accuracy: 0.9536\n",
            "Epoch: 53, Accuracy: 0.9529\n",
            "Epoch: 54, Accuracy: 0.9504\n",
            "Epoch: 55, Accuracy: 0.9502\n",
            "Epoch: 56, Accuracy: 0.951\n",
            "Epoch: 57, Accuracy: 0.9541\n",
            "Epoch: 58, Accuracy: 0.9522\n",
            "Epoch: 59, Accuracy: 0.9537\n",
            "Epoch: 60, Accuracy: 0.9533\n",
            "Epoch: 61, Accuracy: 0.954\n",
            "Epoch: 62, Accuracy: 0.9544\n",
            "Epoch: 63, Accuracy: 0.9532\n",
            "Epoch: 64, Accuracy: 0.9541\n",
            "Epoch: 65, Accuracy: 0.9539\n",
            "Epoch: 66, Accuracy: 0.9535\n",
            "Epoch: 67, Accuracy: 0.9535\n",
            "Epoch: 68, Accuracy: 0.9541\n",
            "Epoch: 69, Accuracy: 0.9565\n",
            "Epoch: 70, Accuracy: 0.9528\n",
            "Epoch: 71, Accuracy: 0.956\n",
            "Epoch: 72, Accuracy: 0.9552\n",
            "Epoch: 73, Accuracy: 0.9537\n",
            "Epoch: 74, Accuracy: 0.9517\n",
            "Epoch: 75, Accuracy: 0.9544\n",
            "Epoch: 76, Accuracy: 0.9535\n",
            "Epoch: 77, Accuracy: 0.956\n",
            "Epoch: 78, Accuracy: 0.953\n",
            "Epoch: 79, Accuracy: 0.9531\n",
            "Epoch: 80, Accuracy: 0.9547\n",
            "Epoch: 81, Accuracy: 0.9543\n",
            "Epoch: 82, Accuracy: 0.9546\n",
            "Epoch: 83, Accuracy: 0.9554\n",
            "Epoch: 84, Accuracy: 0.9549\n",
            "Epoch: 85, Accuracy: 0.9551\n",
            "Epoch: 86, Accuracy: 0.9519\n",
            "Epoch: 87, Accuracy: 0.9541\n",
            "Epoch: 88, Accuracy: 0.9543\n",
            "Epoch: 89, Accuracy: 0.9553\n",
            "Epoch: 90, Accuracy: 0.9562\n",
            "Epoch: 91, Accuracy: 0.9525\n",
            "Epoch: 92, Accuracy: 0.9513\n",
            "Epoch: 93, Accuracy: 0.9535\n",
            "Epoch: 94, Accuracy: 0.9535\n",
            "Epoch: 95, Accuracy: 0.9515\n",
            "Epoch: 96, Accuracy: 0.953\n",
            "Epoch: 97, Accuracy: 0.9513\n",
            "Epoch: 98, Accuracy: 0.9532\n",
            "Epoch: 99, Accuracy: 0.9542\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}